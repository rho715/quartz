---
title: _. Airflow Trigger Next DAG
tags:
  - airflow
  - python
---
# ì‘ì—… ë°°ê²½
- GCP Composerë¡œ ë§¤ ë¶„ CDN ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ì—¬ BigQueryë¡œ í…Œì´ë¸”ì„ ìƒì„±í•˜ê³  ìˆì—ˆìŒ
- ë¬¸ì œëŠ” ì´ì œ í†µê³„Â DBì„œë²„ ê³¼ë¶€í•˜ë¡œ ì¼ë¶€Â API requestê°€Â timeoutÂ í˜¹ì€Â gwì§€ì—°ì´ ë°œìƒí•˜ëŠ” ê²½ìš°ê°€ ìˆì—ˆëŠ”ë° 
- ê¸°ì¡´ì—ëŠ” TASKê°€ ì‹¤íŒ¨í•˜ë©´ ìŠ¬ë™ ì±„ë„ë¡œ ì•ŒëŒì˜¤ëŠ” êµ¬ì¡°ë¼ ì–´ë§ˆë¬´ì‹œí•˜ê²Œ ë©”ì„¸ì§€ê°€ ë§ì´ ì˜¤ê²Œ ë¨ ![[Screenshot 2023-11-02 at 1.59.52 PM.png]]
- í•˜ì—¬ ì½”ë“œ ë¦¬íŒ©í† ë§ ì‘ì—…ì„ ê²°ì •

# ìˆ˜ì • í¬ì¸íŠ¸
- HTTP request ì—ëŸ¬ í•¸ë“¤ë§
	- íŒŒì´ì¬ ë°°ìš°ê³  ì–¼ë§ˆ ì•ˆë˜ì–´ì„œ ì‘ì—…í•œ ì½”ë“œë¼ retry, exceptionë“± ë‹¤ ì²˜ë¦¬ ì•ˆë˜ì–´ìˆìŒ
- ì§ë ¬ -> ë³‘ë ¬
	- Go langì„ ë°°ìš°ë©´ì„œ Goroutineì„ ë°°ìš°ê³  ë™ë£Œê°€ pythonë„ ë¹„ìŠ·í•œ coroutineì´ ìˆë‹¤í•˜ì—¬ API í˜¸ì¶œì„ ë³‘ë ¬ì²˜ë¦¬
- ì•ŒëŒ ë°œì†¡ ì²˜ë¦¬ 
	- AS-IS: Task ë‹¨ìœ„ ê¸°ì¤€ ì—…ë¬´ ì²˜ë¦¬
		- TASK1: API ì¡°íšŒ -> raw ë°ì´í„° DF ì²˜ë¦¬ -> DF BigQuery ì…ë ¥
		- TASK2: raw JSON ë°ì´í„°ì—ì„œ key & value ì¶”ì¶œí•˜ì—¬ columní™” ì‹œì¼œ ëŒ€ì‹œë³´ë“œìš© BigQuery í…Œì´ë¸”ì— ì…ë ¥
		- Alarm: TASK1 or TASK2 ì—ì„œ Task failureê°€ ë°œìƒí•˜ë©´ ìŠ¬ë™ ì•Œë¦¼ ë°œì†¡
	- TO-BE: DAG ë‹¨ìœ„ ê¸°ì¤€ ì—…ë¬´ ì²˜ë¦¬
		- DAG1: API ë³‘ë ¬ ì¡°íšŒ -> raw ë°ì´í„° DF ì²˜ë¦¬ -> DF BigQueryì…ë ¥
		- DAG2: DAG1ì—ì„œ BigQueryì— ì…ë ¥ëœ raw JSON ë°ì´í„° ì¡°íšŒ & key & value ì¶”ì¶œí•˜ì—¬ columní™” ì‹œì¼œ ëŒ€ì‹œë³´ë“œìš© BigQueryì— ì…ë ¥ -> DAG3 Trigger
		- DAG3: DAG2ì—ì„œ ìƒì„±ëœ ë°ì´í„° ì¡°íšŒ & ë°ì´í„° ì—†ì„ì‹œ ìŠ¬ë™ ì•ŒëŒ ë°œì†¡

> [!note]
> - TASK ë‹¨ìœ„ì—ì„œ DAGë‹¨ìœ„ë¡œ ë³€ê²½í•œì´ìœ ëŠ” ë°ì´í„° ì…ë ¥ ì‹œë„ì™€ ë³„ë„ë¡œ ìŠ¬ë™ ì•ŒëŒì„ ê»ë‹¤ í‚¤ê¸° ìœ„í•¨ 

# ì½”ë“œ
## ë””ë ‰í† ë¦¬ êµ¬ì¡°
```
{gcs composer bucket}/dags/visual/prd01/
â”œâ”€â”€ ğŸ—‚ cdn                  
â”‚ â”œâ”€â”€ ğŸÂ visual_cdn_log.py                          : DAG2 
â”‚ â”œâ”€â”€ ğŸÂ visual_cdn_raw.py                          : DAG1                       
â”‚ â””â”€â”€ ğŸÂ visual_cdn_slack_condition_alert.py        : DAG3                      
â”œâ”€â”€ ğŸ—‚ dag_utils              
â”‚ â”œâ”€â”€ ğŸÂ common.py  
â”‚ â”œâ”€â”€ ğŸÂ constant.py  
â”‚ â”œâ”€â”€ ğŸÂ gcp_bigquery.py
â”‚ â”œâ”€â”€ ğŸÂ cdn_api_function.py                        : handle api data
â”‚ â”œâ”€â”€ ğŸÂ cdn_table_config.py  
â”‚ â””â”€â”€ ğŸÂ slack_util.py    
â””â”€â”€ env.py
```

## API JSON FORMAT
```json
# domain list Response
{
    "reqid": "asdfasdf", // ìš”ì²­ id
    "reqtime": "2021-04-12 10:00:00",  // ìš”ì²­ ì‹œê°„
    "rtcode": "200", // ì‘ë‹µ ìƒíƒœ ì½”ë“œ
    "data": [ // ê¶Œí•œì— í•´ë‹¹í•˜ëŠ” ë„ë©”ì¸ ë¦¬ìŠ¤íŠ¸
        "apitest.example.net"
    ],
    "metric": [
        "domain_list"
    ]
}
```

```json
# domain Response
{
    "rtcode": "200", // ì‘ë‹µ ìƒíƒœ ì½”ë“œ
    "reqid": "asdfasdf", // ìš”ì²­ id
    "reqtime": "2021-04-12 10:00:00",  // ìš”ì²­ ì‹œê°„
    "domain": "apitest.example.net", // ì¡°íšŒ ë„ë©”ì¸
    "step": "5m", // ì¡°íšŒ ë‹¨ìœ„
    "metric": [ // ë°ì´í„° ì¢…ë¥˜ (ë°°ì—´ ìˆœì„œ)
        "date",
        "error_ratio"
    ],
    "data": [ // ì‹œê°„ë³„ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        [
            1618110000 // Unix Time (int)
            0 //  ì—ëŸ¬ìœ¨(float)
        ],
        [
            1618110300,
            0
        ],
        ...
   ]
}
```

## ì½”ë“œ íŒŒì¼
<details>
<summary> <code> cdn_api_function.py </code> </summary>
<div markdown="1">

```python

import requests  
from requests.exceptions import Timeout, HTTPError  
import time  
import json  
from concurrent.futures import ThreadPoolExecutor, as_completed  
import pandas as pd  
  
# Constants for API URLs and token  
API_URL = 'https://api.example.net/v2/cdn/'  
TOKEN = 'asdfajsldfjasf'  
  
# Common headers  
HEADERS = {  
    'Authorization': f'Token {TOKEN}'  
}  
  
from datetime import datetime  
import pytz  
  
def convert_unix_to_kst(unix_time):  
    kst = pytz.timezone('Asia/Seoul')  
    time_kst = datetime.fromtimestamp(unix_time, kst)  
    return time_kst  
  
  
def get_api_data(session, api_url, headers, params=None, timeout=5, max_retries=3):  
    """Attempts to get data from the API."""  
    retry_delay = 1  # Start with a 1-second delay  
    for attempt in range(max_retries):  
        try:  
            response = session.get(api_url, headers=headers, params=params, timeout=timeout)  
            response.raise_for_status()  
            return response.json()  
        except Timeout:  
            print(f"Request timed out for {api_url}. Retrying in {retry_delay} seconds.")  
            time.sleep(retry_delay)  
            retry_delay *= 2  # Exponential backoff  
        except HTTPError as e:  
            print(f"HTTP error: {e} for {api_url}. No retry.")  
            break  # No retry for HTTP errors other than timeouts  
        except Exception as e:  
            print(f"An error occurred: {e} for {api_url}. No retry.")  
            break  # No retry for other types of errors  
    return None  
  
  
def modify_json(data):  
    """Modifies the JSON data to be more readable."""  
    data_to_transform = data['data'][0]  
    clean_json = dict(zip(data['metric'], data_to_transform))  
    clean_json.pop('date', None)  
    return clean_json  
  
  
def get_domain_data(session, api_url, headers, domain, unix_timestamp):  
    """Retrieves data for a given domain."""  
    params_base = {  
        'date': unix_timestamp,  
        'domain_name': domain,  
        'step': '1m',  
    }  
  
    # Creating a dictionary of API calls we want to make  
    calls = {  
        'traffic': (api_url + 'traffic', params_base),  
        'status': (api_url + 'status', {**params_base, 'show': 'detail'}),  
        'cache': (api_url + 'cache', params_base),  
    }  
  
    results = {}  
  
    # Execute the API calls concurrently  
    with ThreadPoolExecutor() as executor:  
        future_to_api_name = {  
            executor.submit(get_api_data, session, url, headers, params): name for name, (url, params) in calls.items()  
        }  
        for future in as_completed(future_to_api_name):  
            api_name = future_to_api_name[future]  
            try:  
                data = future.result()  
                results[api_name] = modify_json(data)  
            except Exception as e:  
                print(f"{api_name} generated an exception: {e}")  
  
    combined_results = {}  
    for api_name, data in results.items():  
        combined_results.update(data)  
  
    return combined_results  
  
  
def get_data_as_dataframe(unix_timestamp: int):  
    start_time = time.time()  
    """Function that returns data for all domains as a pandas DataFrame."""  
    data_records = []  
  
    # Use a Session object to persist certain parameters across requests  
    with requests.Session() as session:  
        domain_data = get_api_data(session, API_URL + 'domain_list', HEADERS)  
  
        if domain_data and domain_data.get('data'):  
            domains = domain_data['data']  
            with ThreadPoolExecutor() as executor:  
                # Submit all domain data retrieval jobs to the executor  
                futures = {executor.submit(get_domain_data, session, API_URL, HEADERS, domain, unix_timestamp): domain  
                           for domain in domains}  
  
                # As each job completes, process the results  
                for future in as_completed(futures):  
                    domain = futures[future]  
                    try:  
                        combined_results = future.result()  
                        # Append a record for each domain  
                        data_records.append({  
                            'unix_timestamp': unix_timestamp,  
                            'domain_name': domain,  
                            'data': json.dumps(combined_results)  
                        })  
                    except Exception as e:  
                        print(f"An error occurred for domain {domain}: {e}")  
  
    end_time = time.time()  
    print(f"Total time: {end_time - start_time} seconds")  
  
    # Convert the list of records into a DataFrame  
    df = pd.DataFrame(data_records)  
    return df

```
</div>
</details>
<details>
<summary> <code> visual_cdn_raw.py </code> </summary>
<div markdown="1">

```python
  
#############################  LIBRARIES  
import os  
from datetime import datetime, timedelta  
import json  
  
from airflow import DAG  
from airflow.operators.python_operator import PythonOperator  
from airflow.models import Variable  
  
from visual.prd01 import env  
from visual.prd01.dag_utils.slack_cloudfn import SlackAlert  
from visual.prd01.dag_utils import common  
from visual.prd01.dag_utils.gcp_bigquery import save_to_bq, run_query  
from visual.prd01.dag_utils.skbcdn_api_function import convert_unix_to_kst, get_data_as_dataframe  
from visual.prd01.dag_utils.skbcdn_table_config import {company}_raw_media_traffic_skbcdn  
  
SLACK_INFO = Variable.get(env.airflow_alert_key, None)  
SLACK_INFO = json.loads(SLACK_INFO)  
alert = SlackAlert(channel=SLACK_INFO.get('channel'), slack_info=SLACK_INFO)  
  
LOC = 'asia-northeast3'  
default_args = {  
    'owner': 'rho715@{company}.com',  
    'depends_on_past': False,  
    'email_on_failure': False,  
    'email_on_retry': False,  
    # 'on_failure_callback': alert.slack_fail_alert  
}  
  
from visual.prd01.dag_utils.{company}_cdn_media_traffic import MediaTrafficSKB  
  
############################# BQ ì„¤ì • ë¶€ë¶„  
project_id = common.get_current_project_id()  
  
############################# BQ ì„¤ì • ë¶€ë¶„ - {company}_raw.media_traffic_skbcdndestination_skbcdn = f"{project_id}.{company}_raw.media_traffic_skbcdn"  
job_config_skbcdn = {company}_raw_media_traffic_skbcdn  
  
  
############################# ë°ì´í„° í˜¸ì¶œ ë¶€ë¶„  
def set_time(**kwargs):  
  
    # time_utc = datetime.now() - timedelta(minutes=5)  
    time_utc = kwargs.get('logical_date') - timedelta(minutes=5)  
    time_without_seconds = time_utc.replace(second=0, microsecond=0)  
    unix_time = int(time_without_seconds.timestamp())  
  
    unix_time_kst = convert_unix_to_kst(unix_time)  
  
    print("Processing Time (UTC): ", time_without_seconds)  
    print("Processing Time (KST): ", unix_time_kst)  
    print("unix_time: ", unix_time)  
  
    return_time = {  
        'unix_time': unix_time,  
        'unix_time_kst': unix_time_kst  
    }  
    return return_time  
  
  
def process_media_traffic_{company}_raw(**kwargs):  
    owner = default_args['owner']  
  
    return_time = kwargs.get('ti').xcom_pull(task_ids=['set_unix_time'])  
    return_time = return_time[0]  
    unix_time, unix_time_kst = (return_time.get('unix_time'), return_time.get('unix_time_kst'))  
  
    df = get_data_as_dataframe(unix_time)  
  
    query = f"""  
    DELETE `{project_id}.{company}_raw.media_traffic_skbcdn`  
    WHERE ap_timestamp = '{unix_time_kst}'  
    """  
    run_query(owner, query)  
  
    # save dataframe  
    kwargs_df = {'dataframe': df, 'destination': kwargs['destination'], 'job_config': kwargs['job_config']}  
    save_to_bq(**kwargs_df)  
  
    return_time = {  
        'str_s_time': str(unix_time_kst),  
        'str_e_time': str(unix_time_kst + timedelta(minutes=1))  
    }  
    return return_time  
  
  
# ############################################################################################################################## DAGS : haystack #####  
DAG_ID = os.path.basename(__file__).replace(".pyc", "").replace(".py", "")  
with DAG(  
    dag_id=f'{env.prefix}_{DAG_ID}_v1.0.0',  
    default_args=default_args,  
    schedule_interval='10 * * * *',  
    start_date=datetime(2023, 11, 3, 0, 0, 0),  
    max_active_runs=3,  
    catchup=True,  
    is_paused_upon_creation=True,  
    tags=['api', 'skbcdn', '{company}_raw'],  
) as dag:  
    set_unix_time = PythonOperator(  
        task_id='set_unix_time',  
        provide_context=True,  
        python_callable=set_time,  
        dag=dag,  
        execution_timeout=timedelta(minutes=15),  
        retries=3,  
        retry_delay=timedelta(seconds=10)  
    )  
    prd_{company}_raw_skbcdn = PythonOperator(  
        task_id='prd_{company}_raw_skbcdn_insert',  
        provide_context=True,  
        op_kwargs=default_args,  
        python_callable=process_media_traffic_{company}_raw,  
        dag=dag,  
        execution_timeout=timedelta(minutes=15),  
        retries=3,  
        retry_delay=timedelta(seconds=30)  
    )  
    set_unix_time >> prd_{company}_raw_skbcdn

```
</div>
</details>
<details>
<summary> <code> visual_cdn_log.py </code> </summary>
<div markdown="1">

```python
  
#############################  LIBRARIES  
import os  
from datetime import datetime, timedelta  
import json  
  
from airflow import DAG  
from airflow.operators.python_operator import PythonOperator  
from airflow.models import Variable  
from airflow.models import DagModel  
from airflow.operators.trigger_dagrun import TriggerDagRunOperator  
  
from visual.prd01 import env  
from visual.prd01.dag_utils.slack_cloudfn import SlackAlert  
from visual.prd01.dag_utils import common  
from visual.prd01.dag_utils.gcp_bigquery import run_query  
from visual.prd01.dag_utils.skbcdn_table_config import {company}_log_media_traffic_skbcdn  
  
SLACK_INFO = Variable.get(env.airflow_alert_key, None)  
SLACK_INFO = json.loads(SLACK_INFO)  
alert = SlackAlert(channel=SLACK_INFO.get('channel'), slack_info=SLACK_INFO)  
  
LOC = 'asia-northeast3'  
default_args = {  
    'owner': 'rho715@{company}.com',  
    'depends_on_past': False,  
    'email_on_failure': False,  
    'email_on_retry': False,  
    # 'on_failure_callback': alert.slack_fail_alert  
}  
  
  
############################# BQ ì„¤ì • ë¶€ë¶„  
project_id = common.get_current_project_id()  
DATE_FORMAT = '%Y-%m-%dT%H:%M:00Z'  
  
############################# ë°ì´í„° í˜¸ì¶œ ë¶€ë¶„  
def set_time(**kwargs):  
  
    # time_utc = datetime.now() - timedelta(minutes=5)  
    time_utc = kwargs.get('logical_date') - timedelta(minutes=8)  
  
    str_s_time = common.get_arg(kwargs, 'str_s_time')  # KST  
    if not str_s_time:  
        str_s_time = time_utc + timedelta(hours=9)  
        str_s_time = str_s_time.strftime(DATE_FORMAT)  
  
    str_e_time = common.get_arg(kwargs, 'str_e_time')  # KST  
    if not str_e_time:  
        str_e_time = time_utc + timedelta(hours=9)  
        str_e_time = str_e_time.strftime(DATE_FORMAT)  
  
    print(" ===== START TIME :: " + str_s_time)  
    print(" ===== END TIME   :: " + str_e_time)  
  
    return_time = {  
        'str_s_time': str_s_time,  
        'str_e_time': str_e_time  
    }  
    return return_time  
  
  
def process_media_traffic_{company}_log(**kwargs):  
    owner = default_args['owner']  
  
    return_time = kwargs.get('ti').xcom_pull(task_ids=['set_unix_time'])  
    return_time = return_time[0]  
    str_s_time, str_e_time = (return_time.get('str_s_time'), return_time.get('str_e_time'))  
  
    start_time = datetime.strptime(str_s_time, DATE_FORMAT)  
    end_time = datetime.strptime(str_e_time, DATE_FORMAT)  
  
    while start_time < end_time:  
        yyyy_mm_dd = start_time.strftime('%Y-%m-%d')  
        yyyymmdd = yyyy_mm_dd.replace('-', '')  
  
        start_time_loop = start_time #2022-08-05 00:00:00  
        delta_minutes = timedelta(minutes=1)  
        end_time_loop = start_time_loop + delta_minutes #2022-08-05 01:00:00  
  
        query = {company}_log_media_traffic_skbcdn.format(  
        project_id=project_id,  
        yyyymmdd=yyyymmdd,  
        start_time_loop=start_time_loop,  
        end_time_loop=end_time_loop,  
    )  
        run_query(owner, query)  
  
        start_time = end_time_loop  
  
def is_dag_paused(dag_id):  
    dag_model = DagModel.get_current(dag_id)  
    return dag_model.is_paused  
  
def run_trigger(**kwargs):  
    return_time = kwargs.get('ti').xcom_pull(task_ids=['set_unix_time'])  
    return_time = return_time[0]  
    str_s_time, str_e_time = (return_time.get('str_s_time'), return_time.get('str_e_time'))  
  
    trigger_args = {'str_s_time': str_s_time, 'str_e_time': str_e_time}  
  
    trigger_version = "v1.0.0"  
    target_dag_name = f'{env.prefix}_visual_skbcdn_slack_condition_alert_{trigger_version}'  
  
    # is_dag_paused : True ì´ë©´ dag off, False ì´ë©´ dag on    if is_dag_paused(target_dag_name):  
        print(f"{target_dag_name} - DAG OFF")  
    else:  
        print(f"{target_dag_name} - trigger start")  
        print(f"trigger_args : {trigger_args}")  
        trigger_next_dag = TriggerDagRunOperator(  
            trigger_dag_id=f'{target_dag_name}',  
            task_id='trigger_next_dag',  
            conf=trigger_args  
        )  
        trigger_next_dag.execute(kwargs)  
  
DAG_ID = os.path.basename(__file__).replace(".pyc", "").replace(".py", "")  
with DAG(  
    dag_id=f'{env.prefix}_{DAG_ID}_v1.0.0',  
    default_args=default_args,  
    schedule_interval='10 * * * *',  
    start_date=datetime(2023, 11, 3, 0, 0, 0),  
    max_active_runs=3,  
    catchup=True,  
    is_paused_upon_creation=True,  
    tags=['api', 'skbcdn', '{company}_log'],  
) as dag:  
    set_unix_time = PythonOperator(  
        task_id='set_unix_time',  
        provide_context=True,  
        python_callable=set_time,  
        dag=dag,  
        execution_timeout=timedelta(minutes=15),  
        retries=3,  
        retry_delay=timedelta(seconds=10)  
    )  
    prd_{company}_log_skbcdn = PythonOperator(  
        task_id='prd_{company}_log_skbcdn_insert',  
        provide_context=True,  
        op_kwargs=default_args,  
        python_callable=process_media_traffic_{company}_log,  
        dag=dag,  
        execution_timeout=timedelta(minutes=15),  
        retries=3,  
        retry_delay=timedelta(seconds=30)  
    )  
    task_trigger = PythonOperator(  
        task_id='trigger_skbcdn_monitoring',  
        provide_context=True,  
        op_kwargs=default_args,  
        python_callable=run_trigger,  
        dag=dag,  
        execution_timeout=timedelta(minutes=15),  
        retries=3,  
        retry_delay=timedelta(seconds=10)  
    )  
    set_unix_time >> prd_{company}_log_skbcdn >> task_trigger

```
</div>
</details>
<details>
<summary> <code> visual_cdn_slack_condition_alert.py </code> </summary>
<div markdown="1">

```python

#############################  LIBRARIES  
import os  
from datetime import datetime, timedelta  
  
from airflow import DAG  
from airflow.operators.python_operator import PythonOperator  
  
from visual.prd01 import env  
from visual.prd01.dag_utils.constant import DEFAULT_SLACK  
from visual.prd01.dag_utils.slack_util import SlackUtil  
from visual.prd01.dag_utils.gcp_bigquery import run_query_df  
  
  
alert = SlackUtil(DEFAULT_SLACK.get('channel'))  
  
LOC = 'asia-northeast3'  
default_args = {  
    'owner': 'rho715@{company}.com',  
    'depends_on_past': False,  
    'email_on_failure': False,  
    'email_on_retry': False,  
    'on_failure_callback': alert.send_alert_message  
}  
  
  
def get_args(context):  
    str_s_time = context['dag_run'].conf.get('str_s_time')  
    str_e_time = context['dag_run'].conf.get('str_e_time')  
    return str_s_time, str_e_time  
  
def run_job(**kwargs):  
    str_s_time, str_e_time = get_args(kwargs)  
    owner = default_args['owner']  
    data_check_query = """  
    """.format(str_s_time, str_e_time)  
    df = run_query_df(owner, data_check_query)  
  
    if df:  
        print(df)  
    else:  
        msg = f"No Data Found for : {str_s_time} ~ {str_e_time}"  
        kwargs['task_instance'].xcom_push(key='message', value=msg)  
        alert.send_alert_message(kwargs)  
  
DAG_ID = os.path.basename(__file__).replace(".pyc", "").replace(".py", "")  
with DAG(  
    dag_id=f'{env.prefix}_{DAG_ID}_v1.0.0',  
    default_args=default_args,  
    schedule_interval=None,  
    start_date=datetime(2021, 8, 1, 0, 0, 0),  
    catchup=False,  
    is_paused_upon_creation=True,  
    tags=['api', '{company}_raw', '{company}_log'],  
) as dag:  
    skbcdn_slack_alert = PythonOperator(  
        task_id='skbcdn_slack_alert',  
        provide_context=True,  
        python_callable=run_job,  
        dag=dag,  
        execution_timeout=timedelta(minutes=15),  
        retries=3,  
        retry_delay=timedelta(seconds=10)  
    )  
  
    skbcdn_slack_alert

```
</div>
</details>

# ì‘ì—… log
- ì™œ ë°ì´í„° í”„ë ˆì„ ì²˜ë¦¬ë¥¼ í•˜ì˜€ëŠ”ê°€
	- ì²˜ìŒì— JSON Dataë¥¼ í•˜ë‚˜í•˜ë‚˜ BigQuery í…Œì´ë¸”ë¡œ Insert ì‘ì—…ì„ í•˜ë‹¤ë³´ë‹ˆ Streaming ì¡°ê±´ì— ê±¸ë ¤ ì¶”í›„ ì‚­ì œë‚˜ ìˆ˜ì •ì´ ì•ˆë˜ëŠ” ì´ìŠˆë¡œ ë°ì´í„° ì¬ìƒì„±ì— ì–´ë ¤ì›€ì„ ê²ªìŒ
	- ë”°ë¼ì„œ Pythonìœ¼ë¡œ dfì²˜ë¦¬í•´ì„œ dataframeì„ ì…ë ¥í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í–‰ 